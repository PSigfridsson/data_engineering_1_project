{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "difficult-howard",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark\n",
    "# New API\n",
    "spark_session = SparkSession\\\n",
    "        .builder\\\n",
    "        .master(\"spark://192.168.2.181:7077\") \\\n",
    "        .appName(\"strong_scaling\")\\\n",
    "        .config(\"spark.cores.max\",2)\\\n",
    "        .config(\"spark.driver.memory\", \"1g\")\\\n",
    "        .config(\"spark.default.parallelism\", \"8\")\\\n",
    "        .config(\"spark.dynamicAllocation.enabled\", True)\\\n",
    "        .config(\"spark.shuffle.service.enabled\", True)\\\n",
    "        .config(\"spark.sql.auto.repartition\", True)\\\n",
    "        .getOrCreate()\n",
    "# .config(\"spark.dynamicAllocation.executorIdleTimeout\",\"30s\").config(\"spark.executor.cores\",4)\\\n",
    "# Old API (RDD)\n",
    "spark_context = spark_session.sparkContext\n",
    "#rint(spark_session.sparkContext._conf.get('spark.driver.memory'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "atmospheric-milwaukee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60059988"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark_session.read\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .csv(\"hdfs://192.168.2.181:9000/user/ubuntu/songs_collection/*.csv\")\n",
    "df = df.repartition(8)\n",
    "#    .union(spark_session.read\\.cache()\n",
    "#    .option(\"header\", \"true\")\\\n",
    "#    .csv(\"hdfs://192.168.2.181:9000/user/ubuntu/songs_collection/songs2.csv\"))\\\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "joined-knowing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Release cores and stop application.\n",
    "spark_context.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hybrid-vinyl",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "secondary-airport",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20019996"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "reported-basket",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Release cores and stop application.\n",
    "spark_context.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "molecular-chosen",
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkContext = spark_session.sparkContext\n",
    "rdd=sparkContext.parallelize([1,2,3,4,5])\n",
    "rddCollect = rdd.collect()\n",
    "print(\"Number of Partitions: \"+str(rdd.getNumPartitions()))\n",
    "print(\"Action: First element: \"+str(rdd.first()))\n",
    "print(rddCollect)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
